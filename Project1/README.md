Проект № 1. Анализ публикуемых новостей

Подготовка:
1) Для начала определимся как проект должен выглядеть.
Технологический стек.
Задача проекта - анализ публикуемых новостей. С одной стороны, новости это то, что требует немедленного реагирования. То есть, по сути было бы не плохо обрабатывать данные потоки в режиме реального времени. Но описание витрины говорит нам о том, что нам скорее нужен ретроспективный анализ того, что происходило за сутки, неделю, все время. Следовательно, задержка даже минут в 15 для нас будет не критична. Это не курс валют, где каждый тик может принести большие потери.

Scala или Phython? 
Как уже описано выше нам не требуется в режиме реального времени давать результат, соответственно можно рассмотреть ситуацию со стороны удобства использования и последующей поддержки. В Phython легко подправить код на лету, на случай изменения в стиле оформления RSS, добавить новых источников данных, поменять старые. Тогда как в Scala нужно каждый раз перекомпилировать jar. Опять-таки если сравнивать с анализом тиков колебаний валют, где скорость критична, а источники данных статичны и выбираются тщательно и переход от одного провайдера данных не требуется производить мгновенно. 
Итог: для задачи будет использоваться Python, так как соотвествует задаче, проще в использовании и последующей поддержке.

БД
Самое просто решение PostgreSQL. Его возможностей достаточно для выполнения задания. Но, у меня есть желание проект сделать долгоиграющим, и в последствии использовать для себя. Объем баз будет расти, будет медленней работать. Поэтому выбор пал на Clickhouse. Если это подходит для Yandex, то мне подойдет так же. + Сама платформа весьма актуальна сейчас. Лучше иметь проект на ней.

Оркестратор
Каких-то принципиальных отличий для проекта в них нет, поэтому будет Airflow.

2) Проектирование рабочего процесса
В моем представлении должно быть множество DAGов, каждый из которых выполняет одну функцию:
 1) Группа заданий 1 (task 1) - сбор данных с источников, запись результатов в промежуточные БД
 2) Группа заданий 2 (task 2) - сбор изменений в источниках, пополнение БД свежими данным
 3) Группа заданий 4 (task 4) - аналитическая обработка данных, публикация витрины данных

Выполнение:
1) в Docker установим Clickhouse. 

![clickhouse_up](https://user-images.githubusercontent.com/115062813/218247166-ea45b2ca-970e-401b-a81f-7d532b35c0ac.jpg)

2) Для того чтобы смотреть что у нас происходит в ДБ поставим DBeaver и настроим связь с Clickhouse

![dbeaver](https://user-images.githubusercontent.com/115062813/218247350-e2099831-3719-4b27-a006-75254af08823.jpg)


