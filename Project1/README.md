Проект № 1. Анализ публикуемых новостей

Подготовка:
1) Для начала определимся как проект должен выглядеть.
Технологический стек.
Задача проекта - анализ публикуемых новостей. С одной стороны, новости это то, что требует немедленного реагирования. То есть, по сути было бы не плохо обрабатывать данные потоки в режиме реального времени. Но описание витрины говорит нам о том, что нам скорее нужен ретроспективный анализ того, что происходило за сутки, неделю, все время. Следовательно, задержка даже минут в 15 для нас будет не критична. Это не курс валют, где каждый тик может принести большие потери.

Scala или Phython? 
Как уже описано выше нам не требуется в режиме реального времени давать результат, соответственно можно рассмотреть ситуацию со стороны удобства использования и последующей поддержки. В Phython легко подправить код на лету, на случай изменения в стиле оформления RSS, добавить новых источников данных, поменять старые. Тогда как в Scala нужно каждый раз перекомпилировать jar. Опять-таки если сравнивать с анализом тиков колебаний валют, где скорость критична, а источники данных статичны и выбираются тщательно и переход от одного провайдера данных не требуется производить мгновенно. 
Итог: для задачи будет использоваться Python, так как соотвествует задаче, проще в использовании и последующей поддержке.

БД
Самое просто решение PostgreSQL. Его возможностей достаточно для выполнения задания. Но, у меня есть желание проект сделать долгоиграющим, и в последствии использовать для себя. Объем баз будет расти, будет медленней работать. Поэтому выбор пал на Clickhouse. Если это подходит для Yandex, то мне подойдет так же. + Сама платформа весьма актуальна сейчас. Лучше иметь проект на ней.

Оркестратор
Каких-то принципиальных отличий для проекта в них нет, поэтому будет Airflow.

2) Проектирование рабочего процесса
В моем представлении должно быть множество DAGов, каждый из которых выполняет одну функцию:
 Группа заданий 1  - сбор данных с источников, запись результатов в промежуточные БД
 Группа заданий 2  - сбор изменений в источниках, пополнение БД свежими данным
 Группа заданий 3  - аналитическая обработка данных, публикация витрины данных

Выполнение:
1) в Docker установим Clickhouse. 

![clickhouse_up](https://user-images.githubusercontent.com/115062813/218247166-ea45b2ca-970e-401b-a81f-7d532b35c0ac.jpg)

2) Для того чтобы смотреть что у нас происходит в ДБ поставим DBeaver и настроим связь с Clickhouse


![dbeaver](https://user-images.githubusercontent.com/115062813/218247373-a520b14b-83db-4e40-a614-c7f8ccf0a603.jpg)

3) Так как docker образ airflow не умеет работать с clickhouse у нас есть два выхода: собрать собственный образ с необходимыми библиотеками либо поднять "боевой" инстанс с airflow. Решено было сделать отдельную ВМ на Hyper-V с Ubuntu desktop и на ней поставить Airflow + Python + библиотеки.

![airflowonVM](https://user-images.githubusercontent.com/115062813/218248058-3141bc00-275b-4e98-8fa1-ffcdc6a810ba.jpg)




